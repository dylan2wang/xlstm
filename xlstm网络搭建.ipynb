{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLSTM 网络搭建\n",
    "\n",
    "[paper](https://arxiv.org/abs/1709.08073)\n",
    "\n",
    "说明：\n",
    "\n",
    "原文是建立三个模型，对三个模型的中间一个LSTM层进行特征交叉，每个模型的输入是不同的（体重，步数，睡眠）。本文为了简便，三个模型输入的是相同的数据。即都是 手写字体数据。\n",
    "\n",
    "网络结构图：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/wangruidong/Documents/MachineLearning/Dataset/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/wangruidong/Documents/MachineLearning/Dataset/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/wangruidong/Documents/MachineLearning/Dataset/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/wangruidong/Documents/MachineLearning/Dataset/MNIST/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(55000, 10)\n",
      "Load data finish.\n"
     ]
    }
   ],
   "source": [
    "file_dir = '/Users/wangruidong/Documents/MachineLearning/Dataset/MNIST/'\n",
    "mnist = input_data.read_data_sets(file_dir, one_hot=True)\n",
    "train_img = mnist.train.images\n",
    "train_labels = mnist.train.labels\n",
    "print(train_img.shape)\n",
    "print(train_labels.shape)\n",
    "print('Load data finish.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = 28\n",
    "time_step = 28\n",
    "output_dim = 10\n",
    "batch_size = 15\n",
    "n_hidden = 128 # LSTM 输出维度\n",
    "n_epoch = 20 # 总训练轮数\n",
    "n_batches = 10\n",
    "display_step = 2\n",
    "\n",
    "POS_1 = int(n_hidden / 3)\n",
    "POS_2 = int(POS_1 * 2)\n",
    "POS_3 = int(n_hidden - POS_1 - POS_2)\n",
    "\n",
    "with tf.name_scope('parameters'):\n",
    "    weights = {\n",
    "        'w_out': tf.Variable(tf.random_normal([n_hidden, output_dim], stddev=0.1), name='w_out'),\n",
    "    }\n",
    "\n",
    "    bias = {\n",
    "        'b_out': tf.Variable(tf.random_normal([output_dim], stddev=0.1), name='b_out'),\n",
    "    }\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, time_step, input_dim], name='x_input')\n",
    "    y = tf.placeholder(tf.float32, [None, output_dim], name='y_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dynamic_rnn(x, weights, bias):\n",
    "    # initial x shape = [batch_size, time_step, features]\n",
    "    # ==========> First Layer\n",
    "    with tf.name_scope('first_layer'):\n",
    "        with tf.variable_scope('layer1_m1') as scope:\n",
    "            rnn1_m1 = tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden, forget_bias=1.0, reuse=tf.AUTO_REUSE)\n",
    "            o1_m1, s1_m1 = tf.nn.dynamic_rnn(rnn1_m1,x, dtype=tf.float32, time_major=False)\n",
    "\n",
    "        with tf.variable_scope('layer1_m2') as scope:\n",
    "            rnn1_m2 = tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden, forget_bias=1.0, reuse=tf.AUTO_REUSE)\n",
    "            o1_m2, s1_m2 = tf.nn.dynamic_rnn(rnn1_m2,x, dtype=tf.float32, time_major=False)\n",
    "\n",
    "        with tf.variable_scope('layer1_m3') as scope:\n",
    "            rnn1_m3 = tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden, forget_bias=1.0, reuse=tf.AUTO_REUSE)\n",
    "            o1_m3, s1_m3 = tf.nn.dynamic_rnn(rnn1_m3,x, dtype=tf.float32, time_major=False)\n",
    "    \n",
    "    with tf.name_scope('cross_data'):\n",
    "        with tf.name_scope('split_1'):\n",
    "            split1_o1_m1, split2_o1_m1, split2_o1_m1 = tf.split(o1_m1, [POS_1, POS_2, POS_3], axis=2, name='split_1')\n",
    "        with tf.name_scope('split_2'):\n",
    "            split1_o1_m2, split2_o1_m2, split3_o1_m2 = tf.split(o1_m2, [POS_1, POS_2, POS_3], axis=2, name='split_2')\n",
    "        with tf.name_scope('split_3'):\n",
    "            split1_o1_m3, split2_o1_m3, split3_o1_m3 = tf.split(o1_m3, [POS_1, POS_2, POS_3], axis=2, name='split_3')\n",
    "        with tf.name_scope('concat_1'):\n",
    "            new_o1_m1 = tf.concat([split1_o1_m1, split2_o1_m2, split3_o1_m3], axis=2, name='new_o1_m1')\n",
    "        with tf.name_scope('concat_2'):\n",
    "            new_o1_m2 = tf.concat([split1_o1_m2, split2_o1_m3, split2_o1_m1], axis=2, name='new_o1_m2')\n",
    "        with tf.name_scope('concat_3'):\n",
    "            new_o1_m3 = tf.concat([split1_o1_m3, split2_o1_m1, split3_o1_m2], axis=2, name='new_o1_m3')\n",
    "    \n",
    "    # ==========> Second Layer Cross \n",
    "    with tf.name_scope('second_layer'):\n",
    "        with tf.variable_scope('layer2_m1') as scope:\n",
    "            rnn2_m1 = tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden, forget_bias=1.0, reuse=tf.AUTO_REUSE)\n",
    "            o2_m1, s2_m1 = tf.nn.dynamic_rnn(rnn2_m1,new_o1_m1, dtype=tf.float32, time_major=False)\n",
    "\n",
    "        with tf.variable_scope('layer2_m2') as scope:\n",
    "            rnn2_m2 = tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden, forget_bias=1.0, reuse=tf.AUTO_REUSE)\n",
    "            o2_m2, s2_m2 = tf.nn.dynamic_rnn(rnn2_m2,new_o1_m2, dtype=tf.float32, time_major=False)\n",
    "\n",
    "        with tf.variable_scope('layer2_m3') as scope:\n",
    "            rnn2_m3 = tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden, forget_bias=1.0, reuse=tf.AUTO_REUSE)\n",
    "            o2_m3, s2_m3 = tf.nn.dynamic_rnn(rnn2_m3,new_o1_m3, dtype=tf.float32, time_major=False)\n",
    "    \n",
    "    # ==========> Third Layer\n",
    "    with tf.name_scope('third_layer'):\n",
    "        with tf.variable_scope('layer3_m1') as scope:\n",
    "            rnn3_m1 = tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden, forget_bias=1.0, reuse=tf.AUTO_REUSE)\n",
    "            o3_m1, s3_m1 = tf.nn.dynamic_rnn(rnn3_m1,o2_m1, dtype=tf.float32, time_major=False)\n",
    "\n",
    "        with tf.variable_scope('layer3_m2') as scope:\n",
    "            rnn3_m2 = tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden, forget_bias=1.0, reuse=tf.AUTO_REUSE)\n",
    "            o3_m2, s3_m2 = tf.nn.dynamic_rnn(rnn3_m2,o2_m2, dtype=tf.float32, time_major=False)\n",
    "\n",
    "        with tf.variable_scope('layer3_m3') as scope:\n",
    "            rnn3_m3 = tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden, forget_bias=1.0, reuse=tf.AUTO_REUSE)\n",
    "            o3_m3, s3_m3 = tf.nn.dynamic_rnn(rnn3_m3,o2_m3, dtype=tf.float32, time_major=False)\n",
    "    \n",
    "    with tf.name_scope('merge_3_model'):\n",
    "        out = tf.add(o3_m1[:,-1,:], o3_m2[:,-1,:])\n",
    "        out = tf.add(out, o3_m3[:,-1,:])\n",
    "    \n",
    "    with tf.name_scope('full_connection'):\n",
    "        out = tf.add(tf.matmul(out, weights['w_out']), bias['b_out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 \t Train acc = 0.1333 loss = 2.2831\n",
      "Epoch 4/20 \t Train acc = 0.1600 loss = 2.2750\n",
      "Epoch 6/20 \t Train acc = 0.1000 loss = 2.2829\n",
      "Epoch 8/20 \t Train acc = 0.2867 loss = 2.2678\n",
      "Epoch 10/20 \t Train acc = 0.2933 loss = 2.2573\n",
      "Epoch 12/20 \t Train acc = 0.2800 loss = 2.2444\n",
      "Epoch 14/20 \t Train acc = 0.3733 loss = 2.2378\n",
      "Epoch 16/20 \t Train acc = 0.3733 loss = 2.2122\n",
      "Epoch 18/20 \t Train acc = 0.3400 loss = 2.2040\n",
      "Finish\n"
     ]
    }
   ],
   "source": [
    "# ==========> loss and accuracy\n",
    "with tf.name_scope('predict'):\n",
    "    pre = dynamic_rnn(x, weights, bias)\n",
    "with tf.name_scope('loss'):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pre))\n",
    "with tf.name_scope('train'):\n",
    "    optm = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    _equal = tf.equal(tf.argmax(pre, axis=1), tf.argmax(y, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(_equal, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "train_writer = tf.summary.FileWriter(logdir='logs/', graph=sess.graph)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(n_epoch):\n",
    "    avg_loss, avg_acc = 0.0, 0.0\n",
    "    for i in range(n_batches):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x = batch_x.reshape(batch_size, time_step, input_dim)\n",
    "        feed = {x:batch_x, y:batch_y}\n",
    "        sess.run(optm, feed_dict=feed)\n",
    "        avg_loss += sess.run(cost, feed_dict=feed)\n",
    "        avg_acc += sess.run(accuracy, feed_dict=feed)\n",
    "    avg_loss /= n_batches\n",
    "    avg_acc /= n_batches\n",
    "    if epoch % display_step == 0 and epoch != 0:\n",
    "        print('Epoch %d/%d \\t Train acc = %.4f loss = %.4f' % (epoch, n_epoch, avg_acc, avg_loss))\n",
    "print('Finish')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
